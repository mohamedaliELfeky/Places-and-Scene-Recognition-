{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build-swin-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, Conv2D, LayerNormalization, GlobalAveragePooling1D\n",
        "\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "-V79MsHiolWk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFGS = {\n",
        "    'swin_tiny_224': dict(input_size=(224, 224), window_size=7, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]),\n",
        "    'swin_small_224': dict(input_size=(224, 224), window_size=7, embed_dim=96, depths=[2, 2, 18, 2], num_heads=[3, 6, 12, 24]),\n",
        "    'swin_base_224': dict(input_size=(224, 224), window_size=7, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32]),\n",
        "    'swin_base_384': dict(input_size=(384, 384), window_size=12, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32]),\n",
        "    'swin_large_224': dict(input_size=(224, 224), window_size=7, embed_dim=192, depths=[2, 2, 18, 2], num_heads=[6, 12, 24, 48]),\n",
        "    'swin_large_384': dict(input_size=(384, 384), window_size=12, embed_dim=192, depths=[2, 2, 18, 2], num_heads=[6, 12, 24, 48])\n",
        "}"
      ],
      "metadata": {
        "id": "-xThF8S_okng"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "    \n",
        "    B, H, W, C = x.get_shape().as_list()\n",
        "    \n",
        "    x = tf.reshape(x, shape=[-1, H // window_size,\n",
        "                   window_size, W // window_size, window_size, C])\n",
        "    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
        "    \n",
        "    windows = tf.reshape(x, shape=[-1, window_size, window_size, C])\n",
        "    \n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W, C):\n",
        "    \n",
        "    x = tf.reshape(windows, shape=[-1, H // window_size,\n",
        "                   W // window_size, window_size, window_size, C])\n",
        "    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
        "    \n",
        "    x = tf.reshape(x, shape=[-1, H, W, C])\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def drop_path(inputs, drop_prob, is_training):\n",
        "  \n",
        "    if (not is_training) or (drop_prob == 0.):\n",
        "  \n",
        "        return inputs\n",
        "\n",
        "    # Compute keep_prob\n",
        "    keep_prob = 1.0 - drop_prob\n",
        "\n",
        "    # Compute drop_connect tensor\n",
        "    random_tensor = keep_prob\n",
        "  \n",
        "    shape = (tf.shape(inputs)[0],) + (1,) * \\\n",
        "        (len(tf.shape(inputs)) - 1)\n",
        "  \n",
        "    random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
        "  \n",
        "    binary_tensor = tf.floor(random_tensor)\n",
        "  \n",
        "    output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
        "  \n",
        "    return output"
      ],
      "metadata": {
        "id": "NBtmeICAoy8G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(Layer):\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0., prefix=''):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        \n",
        "        self.fc1 = Dense(hidden_features, name=f'{prefix}/mlp/fc1')\n",
        "        self.fc2 = Dense(out_features, name=f'{prefix}/mlp/fc2')\n",
        "        self.drop = Dropout(drop)\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        x = tf.keras.activations.gelu(x)\n",
        "        \n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "O2jwIFaXo-ze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(Layer):\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0., prefix=''):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.prefix = prefix\n",
        "\n",
        "        self.qkv = Dense(dim * 3, use_bias=qkv_bias,\n",
        "                         name=f'{self.prefix}/attn/qkv')\n",
        "        self.attn_drop = Dropout(attn_drop)\n",
        "        self.proj = Dense(dim, name=f'{self.prefix}/attn/proj')\n",
        "        self.proj_drop = Dropout(proj_drop)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      \n",
        "        self.relative_position_bias_table = self.add_weight(f'{self.prefix}/attn/relative_position_bias_table',\n",
        "                                                            shape=(\n",
        "                                                                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), self.num_heads),\n",
        "                                                            initializer=tf.initializers.Zeros(), trainable=True)\n",
        "\n",
        "        coords_h = np.arange(self.window_size[0])\n",
        "        coords_w = np.arange(self.window_size[1])\n",
        "        coords = np.stack(np.meshgrid(coords_h, coords_w, indexing='ij'))\n",
        "        coords_flatten = coords.reshape(2, -1)\n",
        "        relative_coords = coords_flatten[:, :,\n",
        "                                         None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1).astype(np.int64)\n",
        "\n",
        "        self.relative_position_index = tf.Variable(initial_value=tf.convert_to_tensor(\n",
        "            relative_position_index), trainable=False, name=f'{self.prefix}/attn/relative_position_index')\n",
        "        \n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        B_, N, C = x.get_shape().as_list()\n",
        "      \n",
        "        qkv = tf.transpose(tf.reshape(self.qkv(\n",
        "            x), shape=[-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4])\n",
        "      \n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "      \n",
        "        attn = (q @ tf.transpose(k, perm=[0, 1, 3, 2]))\n",
        "      \n",
        "        relative_position_bias = tf.gather(self.relative_position_bias_table, tf.reshape(\n",
        "            self.relative_position_index, shape=[-1]))\n",
        "       \n",
        "        relative_position_bias = tf.reshape(relative_position_bias, shape=[\n",
        "                                            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1])\n",
        "       \n",
        "        relative_position_bias = tf.transpose(\n",
        "            relative_position_bias, perm=[2, 0, 1])\n",
        "       \n",
        "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "       \n",
        "            nW = mask.get_shape()[0]  # tf.shape(mask)[0]\n",
        "       \n",
        "            attn = tf.reshape(attn, shape=[-1, nW, self.num_heads, N, N]) + tf.cast(\n",
        "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32)\n",
        "       \n",
        "            attn = tf.reshape(attn, shape=[-1, self.num_heads, N, N])\n",
        "       \n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "       \n",
        "        else:\n",
        "       \n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = tf.transpose((attn @ v), perm=[0, 2, 1, 3])\n",
        "        x = tf.reshape(x, shape=[-1, N, C])\n",
        "       \n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "       \n",
        "        return x"
      ],
      "metadata": {
        "id": "7mTp7YkLpXlz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(Layer):\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "\n",
        "        return drop_path(x, self.drop_prob, training)"
      ],
      "metadata": {
        "id": "oH_WVSkcpdOV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(Layer):\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4.,\n",
        "                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path_prob=0., norm_layer=LayerNormalization, prefix=''):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.prefix = prefix\n",
        "\n",
        "        self.norm1 = norm_layer(epsilon=1e-5, name=f'{self.prefix}/norm1')\n",
        "\n",
        "        self.attn = WindowAttention(dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
        "                                    qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, prefix=self.prefix)\n",
        "        \n",
        "        self.drop_path = DropPath(drop_path_prob if drop_path_prob > 0. else 0.)\n",
        "\n",
        "        self.norm2 = norm_layer(epsilon=1e-5, name=f'{self.prefix}/norm2')\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop, prefix=self.prefix)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "\n",
        "            H, W = self.input_resolution\n",
        "\n",
        "            img_mask = np.zeros([1, H, W, 1])\n",
        "\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "\n",
        "            for h in h_slices:\n",
        "\n",
        "                for w in w_slices:\n",
        "\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "\n",
        "                    cnt += 1\n",
        "\n",
        "            img_mask = tf.convert_to_tensor(img_mask)\n",
        "\n",
        "            mask_windows = window_partition(img_mask, self.window_size)\n",
        "\n",
        "            mask_windows = tf.reshape(mask_windows, shape=[-1, self.window_size * self.window_size])\n",
        "            \n",
        "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(mask_windows, axis=2)\n",
        "\n",
        "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            \n",
        "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False, name=f'{self.prefix}/attn_mask')\n",
        "\n",
        "        else:\n",
        "            \n",
        "            self.attn_mask = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        H, W = self.input_resolution\n",
        "      \n",
        "        B, L, C = x.get_shape().as_list()\n",
        "      \n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        \n",
        "        x = self.norm1(x)\n",
        "        x = tf.reshape(x, shape=[-1, H, W, C])\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "        \n",
        "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n",
        "\n",
        "        else:\n",
        "\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "\n",
        "        x_windows = tf.reshape(x_windows, shape=[-1, self.window_size * self.window_size, C])\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = tf.reshape(attn_windows, shape=[-1, self.window_size, self.window_size, C])\n",
        "\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W, C)\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "\n",
        "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n",
        "\n",
        "        else:\n",
        "            \n",
        "            x = shifted_x\n",
        "\n",
        "        x = tf.reshape(x, shape=[-1, H * W, C])\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "2FTTVxFqqPLO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(Layer):\n",
        "\n",
        "    def __init__(self, input_resolution, dim, norm_layer=LayerNormalization, prefix=''):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.reduction = Dense(2 * dim, use_bias=False, name=f'{prefix}/downsample/reduction')\n",
        "\n",
        "        self.norm = norm_layer(epsilon=1e-5, name=f'{prefix}/downsample/norm')\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.get_shape().as_list()\n",
        "      \n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = tf.reshape(x, shape=[-1, H, W, C])\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "       \n",
        "       \n",
        "        x = tf.concat([x0, x1, x2, x3], axis=-1)\n",
        "        x = tf.reshape(x, shape=[-1, (H // 2) * (W // 2), 4 * C])\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TC9SeE5XquAi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "02qBFwKWoSxf"
      },
      "outputs": [],
      "source": [
        "class BasicLayer(Layer):\n",
        "    \n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path_prob=0., norm_layer=LayerNormalization, downsample=None, use_checkpoint=False, prefix=''):\n",
        "      \n",
        "        super().__init__()\n",
        "    \n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks = tf.keras.Sequential([SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                           num_heads=num_heads, window_size=window_size,\n",
        "                                           shift_size=0 if (\n",
        "                                               i % 2 == 0) else window_size // 2,\n",
        "                                           mlp_ratio=mlp_ratio,\n",
        "                                           qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                           drop=drop, attn_drop=attn_drop,\n",
        "                                           drop_path_prob=drop_path_prob[i] if isinstance(\n",
        "                                               drop_path_prob, list) else drop_path_prob,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           prefix=f'{prefix}/blocks{i}') for i in range(depth)])\n",
        "        if downsample is not None:\n",
        "     \n",
        "            self.downsample = downsample(\n",
        "                input_resolution, dim=dim, norm_layer=norm_layer, prefix=prefix)\n",
        "        else:\n",
        "            \n",
        "            self.downsample = None\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "      \n",
        "            x = self.downsample(x)\n",
        "      \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(Layer):\n",
        "  \n",
        "    def __init__(self, img_size=(224, 224), patch_size=(4, 4), in_chans=3, embed_dim=96, norm_layer=None):\n",
        "  \n",
        "        super().__init__(name='patch_embed')\n",
        "  \n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, name='proj')\n",
        "        \n",
        "        if norm_layer is not None:\n",
        "        \n",
        "            self.norm = norm_layer(epsilon=1e-5, name='norm')\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            self.norm = None\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        B, H, W, C = x.get_shape().as_list()\n",
        "        \n",
        "        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        \n",
        "        x = self.proj(x)\n",
        "        \n",
        "        x = tf.reshape(x, shape=[-1, (H // self.patch_size[0]) * (W // self.patch_size[0]), self.embed_dim])\n",
        "        \n",
        "        if self.norm is not None:\n",
        "            \n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Jmx18RiDod2v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, model_name='swin_tiny_patch4_window7_224', include_top=False,\n",
        "                 img_size=(224, 224), patch_size=(4, 4), in_chans=3, num_classes=1000,\n",
        "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
        "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=LayerNormalization, ape=False, patch_norm=True,\n",
        "                 use_checkpoint=False, **kwargs):\n",
        "      \n",
        "        super().__init__(name=model_name)\n",
        "\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "      \n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "      \n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, \n",
        "                                      embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)\n",
        "\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        # absolute postion embedding\n",
        "        if self.ape:\n",
        "\n",
        "            self.absolute_pos_embed = self.add_weight('absolute_pos_embed', shape=(1, num_patches, embed_dim), initializer=tf.initializers.Zeros())\n",
        "\n",
        "        self.pos_drop = Dropout(drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        dpr = [x for x in np.linspace(0., drop_path_rate, sum(depths))]\n",
        "\n",
        "        # build layers\n",
        "        self.basic_layers = tf.keras.Sequential([BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
        "                                                input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
        "                                                                  patches_resolution[1] // (2 ** i_layer)),\n",
        "                                                depth=depths[i_layer],\n",
        "                                                num_heads=num_heads[i_layer],\n",
        "                                                window_size=window_size,\n",
        "                                                mlp_ratio=self.mlp_ratio,\n",
        "                                                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                                                drop_path_prob=dpr[sum(depths[:i_layer]):sum(\n",
        "                                                    depths[:i_layer + 1])],\n",
        "                                                norm_layer=norm_layer,\n",
        "                                                downsample=PatchMerging if (\n",
        "                                                    i_layer < self.num_layers - 1) else None,\n",
        "                                                use_checkpoint=use_checkpoint,\n",
        "                                                prefix=f'layers{i_layer}') for i_layer in range(self.num_layers)])\n",
        "        \n",
        "        self.norm = norm_layer(epsilon=1e-5, name='norm')\n",
        "        self.avgpool = GlobalAveragePooling1D()\n",
        "        \n",
        "        if self.include_top:\n",
        "        \n",
        "            self.head = Dense(num_classes, name='head')\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            self.head = None\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        \n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        if self.ape:\n",
        "        \n",
        "            x = x + self.absolute_pos_embed\n",
        "        \n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        x = self.basic_layers(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        x = self.forward_features(x)\n",
        "        \n",
        "        if self.include_top:\n",
        "        \n",
        "            x = self.head(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "cQHfaCodoZmM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SwinTransformer(model_name='swin_tiny_224', num_classes=1000, include_top=True, pretrained=True, use_tpu=False, cfgs=CFGS):\n",
        "    \n",
        "    cfg = cfgs[model_name]\n",
        "\n",
        "    net = SwinTransformerModel(\n",
        "        model_name=model_name, include_top=include_top, num_classes=num_classes, img_size=cfg['input_size'], window_size=cfg[\n",
        "            'window_size'], embed_dim=cfg['embed_dim'], depths=cfg['depths'], num_heads=cfg['num_heads']\n",
        "    )\n",
        "\n",
        "    net(tf.keras.Input(shape=(cfg['input_size'][0], cfg['input_size'][1], 3)))\n",
        "    \n",
        "    if pretrained is True:\n",
        "    \n",
        "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
        "    \n",
        "        pretrained_ckpt = tf.keras.utils.get_file(model_name, url, untar=True)\n",
        "\n",
        "    else:\n",
        "        \n",
        "        pretrained_ckpt = pretrained\n",
        "\n",
        "    if pretrained_ckpt:\n",
        "        \n",
        "        if tf.io.gfile.isdir(pretrained_ckpt):\n",
        "        \n",
        "            pretrained_ckpt = f'{pretrained_ckpt}/{model_name}.ckpt'\n",
        "\n",
        "        if use_tpu:\n",
        "        \n",
        "            load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "\n",
        "            net.load_weights(pretrained_ckpt, options=load_locally)\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            net.load_weights(pretrained_ckpt)\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "VhWPI8iqoV8b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GfkmZ0m_saDM"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}